---
title: "Introduction to Statistics"
author: Paul Hewson
date: August 5th 2014
output: ioslides_presentation
---

# Introduction to Statistics



## Data

```{r, echo = FALSE, message=FALSE}
#nyc.df <- read.csv("http://dl.dropboxusercontent.com/u/12691674/stat2402/nyc.csv")

library(RCurl)
data <- getURL("http://dl.dropboxusercontent.com/u/12691674/stat2402/nyc.csv", ssl.verifypeer=0L, followlocation=1L)
writeLines(data,'tempnyc.csv')
nyc.df <- read.csv('tempnyc.csv', row.names = 1)
head(nyc.df)
```

## What it looks like

```{r, echo=FALSE}
pairs(nyc.df[,-c(1,2,7)])
```





## Questions?

- Are East restaurants more expensive
- What is the most imporant feature (Decor / Service / Food) in terms of price?
- If I hire a better chef, how much more can I charge for a meal?

## What are we asking from the data?

- Description
- Explanation
- Prediction


## What is the role of statistical inference

- Description - we don't need any statistical model
- For explanation and prediction we need a model
- We regard our data as a sample and we try to make a statement about the population which generated that model
- Big data - population / meta-population




## Models

- $Y \sim Normal(\mu, \sigma^2)$
- $Y = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_i$
with 
$\epsilon_i \sim Normal(0, \sigma^2)$

and even

- $Y_i \sim Binom(n_i, p_i)$
with
$logit(p_i)=\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}$



## Fitting a model

```{r}
nyc.lm1 <- lm(Price ~ Food + Decor + Service, data = nyc.df)
summary(nyc.lm1)
```

## Checking the model assumptions

```{r, echo = FALSE, results='hide'}
par(mfrow = c(2,2))
plot(nyc.lm1)
```

## Having checked the assumptions

- The assumptions look reasonable
- So we can interrogate the model
- Service is not "statistically significant"
- Can we employ Basil Fawlty as a waiter?


## Interpreting statistical significance

- p-values are a measure of evidence against the null
- They are based on simulating a virtual world where the null is tree, and determining the plausibility of the data we actually got 
- Large p-values tell us the data are not inconsistent with the null - maybe we didn't look hard enough / have a large enough sample. 
- Small p-values suggest data are inconsistent with the null but maybe we have too large a sample size / maybe alternative explanations are no better

## Confidence intervals

A 95% confidence interval is wider than a 90% confidence interval

- TRUE or 
- FALSE

## What does "Confidence Interval" mean?

```{r}
output <- rep(0,250)
for (i in 1:1000){
  ## Simulate data from a population
x <- rnorm(1000,5,2)
## Computer upper and lower 95% CI
upper <- mean(x)+1.96*sd(x)/sqrt(1000)
lower <- mean(x)-1.96*sd(x)/sqrt(1000)
## Check CI overlaps "true" value
output[i] <- ifelse(upper>5 && lower<5,1,0)
}
mean(output)
```  



## Using confidence intervals

```{r, echo = FALSE, results = 'hide', message=FALSE}
library(arm)
coefplot(nyc.lm1)
```


# You don't have to do this p-value / CI stuff

- Bayesian inference is extremely popular in modern applications.

- Here's a very simple sketch.   We have some data that could plausibly me modelled as a Normal distribution (coffee dispensed by a vending machine)

- We no longer believe the parameters are fixed but unknown constants.   Instead we need to state a prior distribution  for parameter $\mu$.   For example, I could state that it is $Normal(5,5^2)$.   This decision worries a few people.


## Now imagine we have some data.

- Using Bayes rule we basically form:

$Posterior \propto Prior \times Likehood$


- Everything we want to say about $\mu$ is know summarised by this Posterior distribution.   As it is a pdf we can directly estimate the probahility that $\mu$ is greater than some value.






## An illustration with $n=15$


```{r, echo = FALSE, results='hide'}
sigma2prior <- 5
sigma2y <- 10
muprior <- 5
n <- 15
y <- rnorm(n, 8, 2)
xbar <- mean(y)

sigmapost <- 1 / ( (n / sigma2y) + (1/sigma2prior))
mupost <- sigmapost * ((muprior/sigma2prior) + ((n * xbar)/sigma2y))

hist(y, freq = FALSE, ylim = c(0,0.6), col = "pink", main  = "Data", xlim = c(0,20))
abline(v=mean(y), lwd  = 2, col = "red")

#curve(dnorm(x, muprior, sigma2prior), add= TRUE, col = "blue", lwd = 2, lty = 2)
#curve(dnorm(x, mupost, sigmapost), add = TRUE, col = "red", lwd = 2, lty = 3)
```


## An illustration with $n=15$


```{r, echo = FALSE, results='hide'}
sigma2prior <- 5
sigma2y <- 10
muprior <- 5
n <- 15
y <- rnorm(n, 8, 2)
xbar <- mean(y)

sigmapost <- 1 / ( (n / sigma2y) + (1/sigma2prior))
mupost <- sigmapost * ((muprior/sigma2prior) + ((n * xbar)/sigma2y))

hist(y, freq = FALSE, ylim = c(0,0.6), col = "pink", main  = expression(paste("Data and prior for ", mu)), xlim = c(0,20))
#abline(v=mean(y), lwd  = 2, col = "red")

curve(dnorm(x, muprior, sigma2prior), add= TRUE, col = "blue", lwd = 2, lty = 2)
#curve(dnorm(x, mupost, sigmapost), add = TRUE, col = "red", lwd = 2, lty = 3)
```

## An illustration with $n=15$


```{r, echo = FALSE, results='hide'}
sigma2prior <- 5
sigma2y <- 10
muprior <- 5
n <- 15
y <- rnorm(n, 8, 2)
xbar <- mean(y)

sigmapost <- 1 / ( (n / sigma2y) + (1/sigma2prior))
mupost <- sigmapost * ((muprior/sigma2prior) + ((n * xbar)/sigma2y))

hist(y, freq = FALSE, ylim = c(0,0.6), col = "pink", main  = expression(paste("Data, prior and posterior for ", mu)), xlim = c(0,20))
#abline(v=mean(y), lwd  = 2, col = "red")

curve(dnorm(x, muprior, sigma2prior), add= TRUE, col = "blue", lwd = 2, lty = 2)
curve(dnorm(x, mupost, sigmapost), add = TRUE, col = "red", lwd = 2, lty = 3)
legend("topright", lty = c(2,3), col = c("blue", "red"), legend = c("Prior", "Posterior"))

```




## "Bad" prior but sample size $n=50$


```{r, echo = FALSE, results='hide'}
sigma2prior <- 3
sigma2y <- 2
muprior <- 25
n <- 50
y <- rnorm(n, 8, 2)
xbar <- mean(y)

sigmapost <- 1 / ( (n / sigma2y) + (1/sigma2prior))
mupost <- sigmapost * ((muprior/sigma2prior) + ((n * xbar)/sigma2y))

hist(y, freq = FALSE, ylim = c(0,0.6), col = "pink", main  = expression(paste("Data, prior and posterior for ", mu)), xlim = c(0,30))
#abline(v=mean(y), lwd  = 2, col = "red")

curve(dnorm(x, muprior, sigma2prior), add= TRUE, col = "blue", lwd = 2, lty = 2)
curve(dnorm(x, mupost, sigmapost), add =TRUE, col = "red", lwd = 2, lty = 3)
legend("topright", lty = c(2,3), col = c("blue", "red"), legend = c("Prior", "Posterior"))

```


## Priors

- Don't wish to make light of the problem, careful selection of priors is important
- At the very least in this case we would also want (say) an inverse Gamma distribution for the variance
- But as $n$ increases, the posterior is dominated by the likelihood


## The Posterior

- The posterior is a pdf - in the second case a $Normal(8.06, 0.039)$
- I can directly answer the question what is the probability that $\mu < 8$ as `r pnorm(8, 8.06, 0.039, lower.tail = TRUE)`


# Conclusions

## What is "statistics"

- I would suggest statistics is the art of making a statement about a (meta) population based on a sample of data - either for explanatory or predictive reasons
- Data manipulation is the easiest part of the exercise
- Carefully formulating statistical / probability models, ensuring assumptions are reasonable and drawing conclusions are much harder
- Deming drew a triangle - the kinds of errors we can handle with statisical methods are usually much less severe than the errors (bias) we cannot handle





